% mnras_template.tex 
%
% LaTeX template for creating an MNRAS paper
%
% v3.0 released 14 May 2015
% (version numbers match those of mnras.cls)
%
% Copyright (C) Royal Astronomical Society 2015
% Authors:
% Keith T. Smith (Royal Astronomical Society)

% Change log
%
% v3.0 May 2015
%    Renamed to match the new package name
%    Version number matches mnras.cls
%    A few minor tweaks to wording
%    
%    Beta testing only - never publicly released
%    First version: a simple (ish) template for creating an MNRAS paper

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\documentclass[usenatbib]{mnras}
%\usepackage{newtxtext,newtxmath}
\usepackage[T1]{fontenc}


%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%
\usepackage{graphicx}	% Including figure files
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% AUTHORS - PLACE YOUR OWN COMMANDS HERE %%%%%
\newcommand{\vdag}{(v)^\dagger}
\newcommand\aastex{AAS\TeX}
\newcommand\latex{La\TeX}
\newcommand{\Msun}{\,{\rm M}$_{\odot}$\,}
\newcommand{\kms}{\,{\rm km s}\ifmmode ^{-1}\,\else $^{-1}$\,\fi}
\newcommand{\Mpch}{\,{\rm Mpc}\,\ifmmode h^{-1}\else $h^{-1}$\fi}
\newcommand{\kpch}{\,{\rm kpc}\,\ifmmode h^{-1}\else $h^{-1}$\fi}
\newcommand{\kpc}{\,{\rm kpc}\,}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Cosmic web elements from the $\beta$-skeleton]{The four cosmic web elements from the $\beta$-skeleton}


% The list of authors, and the short list which is used in the headers.
% If you need two or more lines of authors, add an extra line using \newauthor
\author[J. F. Su\'arez-P\'erez et al.]{
J. F. Su\'arez-P\'erez,$^{1}$\thanks{E-mail: jf.suarez@uniandes.edu.co}
Y. Camargo,$^{2}$ 
X.-D. Li,$^{3}$
and J. E. Forero-Romero,$^{1}$
\\
% List of institutions
$^{1}$Departamento de F\'isica, Universidad de los Andes, Cra. 1
No. 18A-10 Edificio Ip, CP 111711, Bogot\'a, Colombia\\ 
$^{2}$Departamento de F\'isica, Universidad Nacional de Colombia -
Sede Bogot\'a, Av. Cra 30 No 45-03, Bogot\'a, Colombia\\ 
$^{3}$School of Physics and Astronomy, Sun Yat-Sen University,
Guangzhou 510297, P.R.China\\ 
}

% These dates will be filled out by the publisher
\date{Accepted XXX. Received YYY; in original form ZZZ}

% Enter the current year, for the copyright statements etc.
\pubyear{2020}

% Don't change these lines
\begin{document}
\label{firstpage}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}
\maketitle

% Abstract of the paper
\begin{abstract}
We present a Machine Learning-based approach to infer the underlying dark matter cosmic web environment of a galaxy distribution from its $\beta$-skeleton graph.
We develop and test our methodology using the cosmological magnetohydrodynamic simulation Illustris-TNG at $z=0$.
We find that a random forest algorithm can use graph-based features to classify a galaxy as belonging to a peak, filament, sheet, or void as defined by the T-Web classification algorithm.
The best match between the galaxies and the dark matter T-Web corresponds to a density field smoothed over scales of $\approx 1.8$ Mpc, a threshold over the eigenvalues of the dimensionless tidal tensor of $0.1$
and galaxy number densities in the range $0.018$-$0.058$ Mpc$^{-3}$.
This methodology results on a global accuracy of $69\%$, which represents an improvement of $18$ percent points over previous work that predicts the T-Web environment from network metrics computed on dark matter haloes.
The completeness changes over filaments ($80\%$), peaks ($63\%$), sheets ($59\%$), and voids ($6\%$).
Higher completeness values are found for environments with a larger number of galaxies.
More extensive tests that take into account lightcone effects and redshift space distortions are left for future work.
We make one of our highest ranking random forest models available on a public repository for future reference and reuse.
\end{abstract}

% Select between one and six entries from the list of approved keywords.
% Don't make up new ones.
\begin{keywords}
large-scale structure of Universe -- dark matter -- methods: miscellaneous
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% BODY OF PAPER %%%%%%%%%%%%%%%%%%
\section{Introduction}
The galaxy distribution on large spatial scales follows a structured 
pattern commonly known as the cosmic web. 
This web can be described as dense peaks connected by anisotropic
filaments and walls woven across vast under-dense voids 
\citep{Bond1996}.  
The emergence of this pattern is understood as the evolution of
initial density fluctuations growing through gravitational instability
\citep{ZelDovich1970,White1987}, a picture that can be followed in
great detail with N-body cosmological simulations
\citep{Schmalzing1999,Vogelsberger2014}.     


There is a great variety of methods to detect
the cosmic web in cosmological simulations.
These methods usually classify the web into four
elements: peak, filament, sheet, or void \citep{Libeskind2018}.
Most of these methods start by building a well-sampled density field from the dark matter simulation particles.

Some methods take as an input the discrete positions that represent galaxies or haloes.
Most of those algorithms focus on the detection of some, not all, cosmic web elements.
An example is the diversity of void-finders
\citep{Platen2007,Neyrinck2008} and filaments-finders
\citep{Novikov2003,Zhang2009,Sousbie2010,Chen2015,Luber2019}.   

The portion of algorithms that aim at finding the four cosmic web elements from the galaxy positions can be roughly divided into two types.
The first type interpolates galaxy positions on a grid to process this number density field in the same way as a dark matter density field \citep{Eardley2015,Alpaslan2016,Tojeiro2017,Shadab2019}.
This approach has a low computational cost, but it is uncertain 
how accurate are their results with respect to the expected cosmic web dark matter distribution.

A second type of algorithm first performs a full dark matter density field reconstruction on the observed galaxy distribution.
Most of these algorithms use Bayesian statistics coupled with Monte
Carlo sampling and N-body
simulations \citep{Jasche2010,Jasche2013a,Bos2014,LeclercqJasche2015,Horowitz2019,Burchett2020};
some others are based on the density distribution around haloes that are associated to galaxy groups \citep{Wang2009,Munoz-Cuartas2011}.
This generic approach can provide accurate dark matter distributions but can
be very expensive from the computational point of view.

Recently, Machine Learning (ML) methods 
have been applied to this problem of finding the four web elements
starting from the dark matter halo distribution \citep{Hui2018, Tsizh2019}.
For instance, 
\citep{Tsizh2019} built networks by linking haloes within a fixed linking length. 
From these graphs, they computed ten different network metrics to feed different ML algorithms and predict the cosmic web for each halo. The true cosmic web environments were quantified using five different cosmic web classifications.

This ML approach avoids the uncertain grid interpolation of
tracers and the expensive dark matter density reconstruction. 
Under this approach of supervised ML classification, 
the algorithm needs a feature dataset (some halo or galaxy
properties), the labels to classify the dataset (the four cosmic web
environments) and an algorithm to link the features to the classes.

The work we present here contributes to this new line of work.
Here we explore to what extent the $\beta$-skeleton graph \citep{Fang2019} can be used to predict the T-Web \citep{Forero-Romero2009} environment classification.
We test two different supervised ML
algorithms (Classification Trees and Random Forests) 
to train from the features and predict the environment.
We perform a thorough exploration of parameter space to quantify the precision and accuracy of the algorithms using the Illustris-TNG simulation \citep{Nelson2019} at redshift $z=0$.

This paper is organized as follows. 
In Section \ref{sec:init} we describe the relevant aspects of the Illustris-TNG
simulations related to our work, the T-Web algorithm,
and the $\beta$-skeleton graph.
In Section \ref{sec:link} we describe the mechanism to link the
$\beta$-skeleton to the T-Web using machine learning algorithms.
There we explain the features and meta-parameters we use, the
classification algorithms and the metrics for model evaluation.  
In Section \ref{sec:results} we present our results. In Section \ref{sec:discussion} we explore a brief discussion comparing our results against \citep{Tsizh2019} to finally summarize our conclusions in Section \ref{sec:conclusions}.

\section{Algorithms and Simulations}\label{sec:init}

\subsection{T-Web Classification}


The tidal web (T-Web) method \citep{Hahn2007, Forero-Romero2009}
classifies the large scale structure into four web types: voids,
sheets, filaments, and peaks.   
This classification is based on the eigenvalues of the deformation
tensor $T_{\alpha\beta}$ computed as the Hessian of the gravitational potential 
\begin{equation}
T_{\alpha\beta}=\frac{\partial^2\psi}{\partial r_{\alpha}r_{\beta}},
\end{equation}
%
where $\psi$ is a normalized gravitational potential that follows the equation
\begin{equation}
    \nabla^2 \psi = \delta,
\end{equation}
%
and $\delta$ is the dark matter overdensity.
This tensor has three real-valued eigenvalues. 
The cosmic web environment is defined by the number of eigenvalues
larger than a threshold value $\lambda_{th}$.
Locations with three eigenvalues larger than $\lambda_{th}$ correspond
to a peak, two to a filament, one to a sheet, and zero to a void.


Computationally speaking, to define an environment in an N-body
simulation we implement the following seven steps: 1) interpolate the
mass particles with a Cloud-In-Cell (CIC) scheme over a grid to
obtain the density, 2) smooth it with an isotropic Gaussian filter,
3) compute the overdensity, 4) find the normalized potential with Fast
Fourier Transform  methods, 5) compute the deformation tensor using
finite differences, 6) find the eigenvalues, and finally 7) count the
number of eigenvalues larger than the threshold $\lambda_{th}$.  

\begin{figure}
\centering
 \includegraphics[scale=0.18]{Figs/p_beta.pdf}
 \caption{Exclusion region to build the $\beta$-skeleton under the Lune-based definition.}  
 \label{fig:beta}
\end{figure}


\begin{figure*}
\centering
 \includegraphics[scale=0.3]{Figs/p_Fig1_.png}%Figs/p_TWeb_bsk.pdf}
 \caption{Comparison between the T-Web of dark matter density field
   (left) and the $\beta$-skeleton (right) computed from the
   distribution of galaxies with $\beta$=1 for TNG300-1.}  
 \label{fig:TWebBsk}
\end{figure*}


\subsection{The $\beta$-skeleton algorithm}


The $\beta$-skeleton is an algorithm that computes a graph over a
distribution of nodes.  
This algorithm depends on a positive and continuous parameter $\beta$
that defines an exclusion region between two nodes.
Here we use the Lune-based definition for the exclusion region \citep{Kirkpatrick1985}.

Two nodes separated a distance $d$ are connected by an edge if and only the exclusion regions defined between the two points is empty.
This exclusion region is a function of $\beta$.
For $\beta<1$, the exclusion region is the intersection of the two  
congruent spheres with diameter $d/\beta$ constructed as shown in Figure \ref{fig:beta}.
For  $\beta=1$, the exclusion region is a sphere with a diameter $d$ and for $\beta>1$ the spheres have diameter $d\beta$.
The $1$-skeleton also receives the name of Gabriel Graph, while the
 $2$-skeleton is also known as the Relative Neighbor Graph (RNG).
More details on the properties of the $\beta$-skeleton as applied to large scale structure data can be found in \cite{Fang2019}.

\subsection{Illustris-TNG}

We test our algorithms on a simulation from the Illustris-TNG project.
This project \citep{Nelson2019} is a series of large,
cosmological gravo-magnetohydrodynamical simulation of galaxy formation. 
These simulations are based on the cosmological standard model
$\Lambda$CDM and uses the \texttt{AREPO} code \citep{Springel2011}.
The simulations start from the cosmology $\Lambda$CDM with the
cosmological parameters: cosmological constant
$\Omega_{\Lambda,0}$=0.6911, matter density $\Omega_{m,0}$=0.3089,
baryonic density $\Omega_{b,0}$=0.0486, normalization
$\sigma_8$=0.8159, spectral index $n_s$=0.9667 and Hubble constant
$H_o$= 100$h$ km s$^{-1}$ Mpc$^{-1}$ with $h$=0.6774, consistent with
the Planck 2015 results \citep{Ade2016}.  

The Illustris-TNG project simulations follows the formation and evolution of stars, black holes jointly with the dark matter and gaseous cosmic web evolution.
The simulation includes sub-grid algorithms to follow the stellar and black hole evolution together with its associated feedback processes \citep{Nelson2019,Springel2018}.
There are 18 different simulations with boxes of sizes $50$ Mpc, $100$ Mpc, and $300$ Mpc that started at $z=127$ and finalized at $z=0$.    
In this work we use the $z=0$ catalog from the largest box at the highest possible resolution: TNG300-1.
This simulation has a volume of 302.6$^3$ Mpc$^3$, with a baryonic mass resolution of $8.8\times 10^{7}$ \Msun and a dark matter mass resolution of $4.7\times  10^{8}$ \Msun \citep{Nelson2019}.
The are approximately 14 million galaxies at $z=0$ in that simulation.
We use the galaxy catalogs from the public release of the TNG300-1 simulation \citep{Pillepich2018a} to build the $\beta$-skeleton graph.
We use the dark matter particles at $z=0$ to obtain the DM T-web classification.
 

\begin{figure*}
        \includegraphics[scale=0.46]{Figs/p_all_features_correlations.pdf}
    \caption{Histograms and correlations curves for the six features used to train the classifiers. These features were extracted from the 1-skeleton graph. The contour lines in the 2D histograms represent the levels that include 39\%, 86\% and 98\% of the data.}
    \label{fig:features}
\end{figure*}

\section{Linking the $\beta$-skeleton to the T-Web}\label{sec:link}

Figure \ref{fig:TWebBsk} illustrates the correlation between the dark matter overdensity field and the $\beta$-skeleton graph computed over the corresponding galaxy distribution. 
This spatial correlation suggests that graph derived features might have
a good chance to predict the dark matter web environment. 
However, there is a variety of free parameters combinations that have to be explored to find the best match between the $\beta$-skeleton and the T-Web.
For instance, the T-Web classification depends on the smoothing scale, $\sigma$, and the threshold parameter, $\lambda_{th}$; the $\beta$-skeleton changes with $\beta$; and the tracers we use to build the $\beta$-skeleton can also change if there is a selection function.
In what follows we explore those aspects.


\subsection{T-Web and $\beta$-skeleton free parameters}

We perform the T-Web computation over a cubic grid with $256^3$ cells. 
This corresponds to a cell size of $1.17$ Mpc.
We explore five different values for the smoothing parameter $\sigma =
0.5$, $1.0$, $1.5$, $2.0$ and $2.5$, expressed in units of the cell size.
For each smoothing length we take six different values for
the eigenvalue threshold
$\lambda_{th}=0.0, 0.1, 0.2, 0.3, 0.4$ and $0.5$. 
For the galaxy selection function we use a simple magnitude cut ($M_{r}<M_{r\rm{lim}}$) on the $r$-band absolute magnitude.
We use four different values for $M_{r\rm{lim}}$: $-18$, $-16$, $-14$ and $-12$, corresponding to number densities of $0.009$, $0.018$, $0.031$ and $0.058$ Mpc$^{-3}$, respectively.
From these galaxies, we compute the skeleton using values of $\beta$ in
the range $1\leq \beta \leq 2$.

We find that the $\beta$ skeleton features for $\beta=1$ are sufficient to make the T-Web environment prediction, i.e. adding features computed for different values of $\beta$ do not significantly improve the results.
Therefore, we keep $\beta=1$ fixed for the results reported in this Paper.

Once these four free different parameters ($\beta$,
$\sigma$, $\lambda_{th}$ and $M_{r\rm{lim}}$) are chosen the $\beta$-skeleton and the T-Web are fixed.  
Next, we define the input features to predict the four cosmic web elements.

\subsection{Galaxy Features}

We use six features for each galaxy to train the ML algorithm:
\begin{enumerate}
\item[1)]
The number of connections minus the global median number of
connections, $\eta = N_c - \bar{N}_c$. 
\item[2)]
The ratio between the average edge length to first neighbors 
normalized by the global average edge length, $\delta=D_{av}/\bar{D}_{av}$. 
\item[3)] 
The logarithm of the pseudo-density over the graph,
  $\varrho=\log_{10}\rho$.   
Where the pseudo-density $\rho$ is defined as the inverse of the volume
computed from the ellipsoid that best fits the distribution of first
neighbors over the graph. 
If a galaxy has less than three neighbors its volume is fixed to be the $D_{av}^3$.
\end{enumerate}
\noindent
We define three more features that we call $\Delta$-features.
Their values are computed as the difference between the corresponding
value in a node and the average value over first neighbor nodes.  
They can be loosely interpreted as a divergence computed over the graph.

\begin{enumerate}
\item[4)] $\Delta$ over the number of connections, $\Delta\eta$.
\item[5)] $\Delta$ over the normalized edge length, $\Delta\delta$.
\item[6)] $\Delta$ over the the pseudo-density, $\Delta\varrho$.
\end{enumerate}

We do not find any significant improvement if the $M_{r}$ magnitudes are included in the features. 
Figure \ref{fig:features} shows the correlations among all features.
The diagonal panels present the distribution of a given parameter.


\subsection{Classification Algorithms}

We present results from two different supervised machine learning algorithms:

\begin{itemize}
    \item Classification Trees (CT). Changing the maximum tree depth: From 1 to 30.
    \item Random Forest (RF). With a fixed maximum tree depth of 10 and changing the Number of estimators from 1 to 100.
\end{itemize}

For some of the models we find that a Support Vector Machine (SVM) gives worse results than a CT or RF classifier.
For that reason we do not pursue a full parameter space exploration with SVM and focus instead on the CT and RF results.


\subsection{Model evaluation}

We partition the simulation box into eight cubic sub-boxes of equal volume.
For each sub-box we compute the $\beta$-skeleton and the related feature parameters.
Afterwards, we discard the galaxies located within 5 Mpc away from the sub-box limits to avoid boundary effects coming from incomplete $\beta$-skeleton information.
The T-Web information comes from the computation over the full box to take advantage of the periodic boundary conditions.

As the \emph{training} dataset we use the features from four sub-boxes that only touch on one vertex.
The \emph{evaluation} dataset is composed by the features from two more sub-boxes.
This is the dataset we use to select what T-Web parameters and algorithm meta-parameters provide the best results.
Once the free parameters and meta-parameters are fixed we use one sub-box as \emph{test} dataset to report the final scores and confusion matrix.
The remaining sub-box is not used in this paper, but is made available in a public repository \footnote{\url{https://github.com/jsuarez314/cosmicweb_bsk}} together with the best ML model. 


As performance metric to compare our models we use the F1 score which is the harmonic mean between the purity (also called precision) and completeness (also called recall).
We compute the F1 score separately for each web environment.
In the Discussion section we also use a global accuracy (the fraction of total correct predictions) in order to compare against the results by \cite{Tsizh2019}.

\begin{figure}
    \includegraphics[scale=0.55]{Figs/p_hist_f1.pdf}
    \caption{Histograms of the average F1 score for the two machine
      learning algorithms (Random Forest (RF) and Classification Trees (CT)). 
      Each average value correspond to a different set of free parameters and metaparameters.}
    \label{fig:methods}
\end{figure}


\begin{figure*}
\centering
    \includegraphics[scale=0.23]{Figs/p_features_Forest_F1_av.pdf}
    \includegraphics[scale=0.23]{Figs/p_features_Forest_F1_av_no_voids.pdf}
    \caption{Parameter and meta-parameter distributions for the 15 RF models with the highest F1 average scores. When the voids are included in the F1 average (upper row) we observe that  the $\lambda_{th}$ meta-parameter jumps between $0.4$ to $0.5$, $\sigma$ is around $1.5$ and $M_{r \rm lim}$ is $-16$
    When voids are excluded from the F1 average  (bottom row) the preferred  $\lambda_{th}$ drops to $0.1$, $M_{r \rm lim}$ also includes values of $-12$ and $\sigma$ stays the same.} 
    \label{fig:features_score}    
\end{figure*}


\begin{table}
\centering
\begin{tabular}{ccc}
\hline
    & F1-score & F1-score              \\
     & RF              & CT              \\
\hline
 Peaks              & 0.53 $\pm$ 0.26 & 0.47 $\pm$ 0.25 \\
 Filaments          & 0.71 $\pm$ 0.05 & 0.65 $\pm$ 0.07 \\
 Sheets            & 0.58 $\pm$ 0.06 & 0.51 $\pm$ 0.11 \\
 Voids               & 0.31 $\pm$ 0.19 & 0.26 $\pm$ 0.17 \\
 Average including Voids    & 0.53 $\pm$ 0.09 & 0.47 $\pm$ 0.09 \\
 Average excluding Voids & 0.61 $\pm$ 0.09 & 0.54 $\pm$ 0.09 \\
\hline
\end{tabular}
\caption{Mean values and standard deviations 
  over the F1 scores for all models.
  The first four rows differentiate the results for each environment.
  The fifth row corresponds to the unweighted average over the four web elements, while the last row is the average over web elements without voids.}
\label{table:elements}
\end{table}


\section{Results}\label{sec:results}

\subsection{ML algorithm performance}

We start by comparing the performance of the different ML algorithms.
Figure \ref{fig:methods} shows the average F1 distribution for
all the classification experiments.
The histograms are discriminated into the two ML algorithms.
From these two models, the results from the RF are better; the F1 value distribution is skewed towards higher values than the distribution from the CT.
From this overall response over all possible models, Random Forest
emerges as the best ML algorithm.
We can confirm this result by performing a more detailed analysis by
taking a look at the results for each cosmic web element separately.

Table \ref{table:elements} summarizes the F1 performance split by cosmic
web element in the first four rows.
The mean values and standard deviations are computed over all the ML experiments. 
From this table, we confirm that the RF classifier performs
consistently better than CT for every web element.
The F1 scores comparison between RF and CT shows that for the four environments the RF produces average F1 scores around 0.05 or higher. 

\subsection{Performance across web elements}

Table \ref{table:elements} also shows that some cosmic web elements 
are easier to predict than others.
Focusing our attention on the RF classifier, the algorithm with the
best results, we readily observe that filaments is the environment
with the best mean F1 score (0.71).
This score is followed by sheets and peaks (0.58 and 0.53,
respectively) and finally by voids (0.31).

Void is the hardest class to predict. 
The reason behind this result is easy to understand. 
There are too few galaxies in voids.
Across all models, less than $2\%$ of all galaxies
are classified as void galaxies.
This has two consequences.
First, the low number of instances makes it hard for the ML algorithm to
define the region in parameter space occupied by voids.
Second, a small number of void galaxies that are misclassified is translated into large changes in the F1 score.

In comparison, Filament is the class with the best score.
The F1 score for filament is the highest of all four
web elements with an average value of $0.71$.
Notably, the F1 dispersion across models is also the lowest corresponding to $0.05$.
Galaxies in filaments are also the most represented class,
approximately $51\%$ of all galaxies are classified as filament galaxies.
This helps to explain that it is easier for the ML algorithm to learn
the characteristics that make a galaxy sit in a filament.
Sheet and Peak have intermediate F1 scores between
those for Void and Filament, with average values of $0.58$ and $0.53$.
The fraction of galaxies in Sheets and Peaks are also intermediate around $22\%$ and $25\%$, respectively.

The failure to classify voids has to be taken into account at the
moment of assigning a global score to a model.
Not taking into account these special results can be misleading.
The reason for that is we want use the average F1 across the four classes to define the best model. 
We  also compute the average F1 across classes, but
\emph{excluding voids}.
This choice is equivalent to using the average F1 across classes but \emph{weighted} by the number of galaxies in each class.
This is what we do in the last two rows in Table \ref{table:elements}.
In what follows we keep in mind this distinction in order to provide a fairer evaluation for each model.


\subsection{Best match between T-Web and $\beta$-skeleton}

We now focus on understanding what parameters produce the best match between the T-Web and the features derived from the $\beta$-skeleton.
In this exploration, we focus on the RF algorithm given that we already know that it 
provides the best classification results.

To do that we choose the best $N=15$ models in terms of their average F1 across classes and see what do they have in common.
Choosing $N=10$ or $N=20$ does not have a great impact on our results. 
However, as we previously saw, including voids or not in the analysis
has deeper consequences that we are going to explore next.

Figure \ref{fig:features_score} summarizes the parameter and meta-parameter
distribution for the best $N=15$ RF models.
The upper raw takes voids into account for the F1 average computation,
while the lower raw does not.
The first two panels show the T-Web parameters
$\lambda_{th}$ and $\sigma$, the third panel corresponds to $M_{r\rm lim}$, the fourth panel is the meta-parameter Number of Estimators ($NE$) in the RF
classifier and finally the last panel shows the average F1 across classes.

Excluding the void F1-score or not has a dramatic influence on on the preferred valued for $\lambda_{th}$. 
If the void F1-score is included in the average F1 the preferred value is high, close to
$\lambda_{th}=0.5$. 
This preferred value drops to $\lambda_{th}=0.1$ if voids are not taken into account in the average score.
Why is this difference relevant?
Pushing $\lambda_{th}$ towards high values makes the voids larger.
That means that more galaxies are included in voids, giving the ML algorithm more instances to train, explaining the rise in the F1 score for $\lambda_{th}=0.5$.

However, the T-Web voids corresponding to $\lambda_{th}=0.5$ are not completely physical: those voids are large and encompass regions with densities several times above the mean density.
In other words, the $\lambda_{th}=0.5$ voids are actually including regions that should be classified as sheets \citep{Bustamante2015, Forero-Romero2009}.

For that reason we decide to exclude the F1-score for voids in judging what model is best.
This choice is equivalent to computing the average F1-score weighted by the number of galaxies in each environment. 
The weighted average score automatically gives less importance to the less represented class, which in this case corresponds to Voids.

We make our choice on a preferred set of parameters among the results from the bottom row in Figure \ref{fig:features_score}.
The choice for $\lambda_{th}$ and $\sigma$ is easy given that there is a single preferred value.
For $M_{r\rm lim}$ we pick the brightest cut of $M_{r\rm lim}=-16$ (to have the dataset with the lowest number density) and $NE=80$ (the meta-parameter that gives the highest F1 score after all the other parameters have been fixed).
We use this model to compute confusion matrix and the feature importance on the \emph{test} dataset.
Choosing any other of the models gives similar results.



\subsection{Confusion Matrix and Feature Importance}

Figure \ref{fig:confusion_matrix} shows the confusion matrix computed on the \emph{test} dataset using our preferred model.
The numbers correspond to the fraction of objects that correspond to
the \verb"Truth" class but are classified into the \verb"Prediction" class.
What are the most common misclassifications?
In first place comes the $91\%$ of void galaxies that are misclassified as sheet galaxies.  
In second place we have the $41\%$ of sheets that are misclassified
as filaments, and finally, the $37\%$ of peaks that are misclassified as filaments.

\begin{figure}
\centering
    \includegraphics[scale=0.35]{Figs/p_confusion_matrix_test.pdf}
\caption{Confusion matrix for the model 
      \{$\lambda_{th}=0.1$, $\sigma=1.5$, $M_{r\rm lim}=-16$,
      $NE$=80\} computed on the test dataset.}
      \label{fig:confusion_matrix}
\end{figure}

\begin{figure}
    \includegraphics[scale=0.29]{Figs/p_features_importance_test.pdf}
    \caption{
      Feature importance in the Random Forest classifier for the model
      \{$\lambda_{th}=0.1$, $\sigma=1.5$, $M_{r\rm lim}=-16$,
      $NE$=80\} computed on the test dataset.}
    \label{fig:feature_importance}
\end{figure}

\begin{figure*}
  \centering 
    \includegraphics[scale=0.28]{Figs/p_environment_predicted.pdf}
    \caption{Spatial distribution for the galaxies classified into the
      four web elements as predicted from the best model (left) and its corresponding truth (right).}
    \label{fig:prediction}
\end{figure*}

The most common missclassifications highlights the difficulty to find void galaxies.
This turns out to be an almost impossible task for one simple reason already mentioned before:
there are very few void galaxies.
The second kind of misclassification shows that the galaxies classified as belonging to a filament, actually belong to neighboring
environments: sheets and peaks.

This misclassification can also be understood in terms of the topological properties of the cosmic web elements. 
Voids are surrounded by sheets (hence the misclassification between the two), sheets have filaments inside them and
filaments in turn have peaks inside them \citep{Cautun2014} (explaning the missclassification of peaks and sheets into filaments). 
In this progression the average density increases and the tidal field
anisotropy goes from being isotropic (voids) to anisotropic (sheets
and filaments) and isotropic (peaks) \citep{Bustamante2015}.


The ranking of correct classifications (the diagonal over the confusion matrix, which also correspond to the completeness) naturally follows the F1 trends shown in Table \ref{table:elements}.
Filaments are the class most correctly classified ($80\%$) followed by
peaks ($63\%$) and sheets ($59\%$).
Void galaxies have the worst results, with only $6\%$ of correct classifications.
This ranking also follows the trend in the fraction of instances belonging to
filaments ($51\%$), peaks ($25\%$), sheets ($22\%$), and voids ($2\%$).
A higher number of instances translates into a better chance for the
algorithm to give a correct classification. 

Figure \ref{fig:feature_importance} shows the feature
importance for the classification.
As it could probably be expected, the average connection length ($\delta$) and the pseudo-density ($\varrho$) turn out to be crucial for the classification.
The other feature with importance similar to the pseudo-density is  $\Delta \delta$, the change in the average edge length.


Figure \ref{fig:prediction} summarizes our results in a qualitative fashion.
It shows the spatial distribution for the galaxies into the four web
elements. 
The left panel correspond to the predictions from the best model, the right panel correspond to the truth.
The visual impression for filament galaxies is the most
similar between truth and prediction, as expected by the quantitative
results presented here.
Sheet galaxies in the prediction appear less clustered
than they should.
The failure in the void classification is clearly visible as galaxy deficit in the prediction.



\section{Discussion}\label{sec:discussion}

\begin{table*}
\centering
\begin{tabular}{c|c|c|c|c|}
\cline{2-5} &
\multicolumn{2}{c|}{{\begin{tabular}[c]{@{}c@{}}Best
      Model (Tsizh et al. 2020) \end{tabular}}} &
\multicolumn{2}{c|}{{\begin{tabular}[c]{@{}c@{}}Best Model
(This Work) \end{tabular}}} \\ \hline
\multicolumn{1}{|c|}{}     & Completeness   &
Tracer \%   & Completeness    & Tracer \%  \\ \hline
\multicolumn{1}{|c|}{Peaks}     & 0.41   & 5.0   & 0.63  & 25.4   \\ 
\multicolumn{1}{|c|}{{Filaments}} & 0.62   & 19.9  & 0.80  & 50.4   \\ 
\multicolumn{1}{|c|}{{Sheets}}    & 0.64   & 38.8  & 0.59  & 22.3   \\ 
\multicolumn{1}{|c|}{{Voids}}     & 0.82   & 36.3  & 0.06  & 1.9    \\\hline 
\end{tabular}
\caption{Completeness (diagonal elements of the confusion matrix) results from the best model in \citet{Tsizh2019} and our work.
Although the global accuracy in both cases is close to $0.70$, the completeness across web elements are different.
This difference is explained by the distinct results from each web environment classification.}
\label{tab:tsizh}
\end{table*}

We focus the following discussion on the comparison  against the results from the similar methodology proposed by \citep{Tsizh2019}.
Their work combines network analysis with ML to predict the environments in the cosmic web. 
In that paper, the nodes are represented by dark matter haloes in a box with length of 290 Mpc extracted from the \texttt{GADGET-2} cosmological simulation \citep{Springel2005}. 
The haloes in the simulation were identified by a friends-of-friends algorithm \citep{Libeskind2018}. 
Their catalog has a total of 281465 haloes with mass range from 10$^{11}h^{-1}$ to 10$^{15}h^{-1}$\Msun, this corresponds to a similar number density in our models with $M_{r\rm lim}=-12$.

They built a networks with different linking lengths in the range $l=2$-$4$ \Mpch.
From the networks they computed 10 different metrics, which, together with the mass and the
peculiar velocity, made up the features for the ML algorithms. 
They also explored five different cosmic web classification algorithms.
As a score metric they use the global accuracy (the ratio of correct predictions to total number of instances).
With that setup they found that the best results were provided by extreme gradient boosting decision trees (\texttt{xgboost}) on the Classic classification method.

In Table \ref{tab:tsizh} we compare the results of their best model against ours.
The completeness is extracted from their reported confusion matrix.
We see that our model gives better results for Peaks and Filaments, while theirs gives better results from Sheets and Voids.

This trend is easy to explain after inspecting the fraction of tracers in each environment for each model.
\cite{Tsizh2019} have $36.4\%$ and $38.8\%$ of halos in voids and sheets, respectively. 
This makes those two environments the most populated, giving the highest completeness.
In our case the most populated environments are Peaks and Filaments, which also translates into the highest completeness in our classification.
The reason for the contrast in populations across environments is that these results come from two different cosmic web classifiers.


To provide a fair comparison against their results, we take a look instead at their T-Web environment predictions, which is the same cosmic web classifier we use.
\cite{Tsizh2019} found a global accuracy of 0.51 (T-Web was the worst predicted method) while we have a higher score of 0.69.
Unfortunately \cite{Tsizh2019} did not report the confusion matrix for T-Web classifications, only the global accuracy is listed in their results.

There are three differences that might explain the better performance of the method presented here. 
First, we make a thorough exploration in parameter space to find what T-Web parameters are the best matched to the graph features.
Second, we have a simulation that allows us  to explore higher number densities, which seems to be required to have a good match with the T-Web environment.
Third, the $\beta$-skeleton, due to its adaptive nature, could provide a better graph representation than a network with a fixed linking length.
However, finding a definite answer on the weight of these three methodological aspects to explain the performance improvement is beyond the scope of this paper.



\section{Conclusions}\label{sec:conclusions}

In this paper, we presented a method to predict the dark matter cosmic web environment of a galaxy from its neighbor graph information.
We used the T-Web as the cosmic web definition \citep{Forero-Romero2009} and the $\beta$-skeleton graph \citep{Fang2019} to describe the relative spatial distribution of the galaxies. 
The link between the T-Web and the $\beta$-graph was done through two different machine-learning algorithms: Classification Trees and Random Forest.
We tested our method using data from the Illustris-TNG simulation \citep{Nelson2019} at $z=0$. 

Using the F1-score as a benchmark, we first found that the Random Forest algorithm provides the best results. Then, we showed that the most accurate predictions between the graph properties and the T-Web environment are provided for a dark matter density field smoothed over a scales $1.5$\Mpch, an eigenvalue threshold $\lambda_{th}=0.1$.
The preferred eigenvalue threshold turns out to be the ballpark range favored in previous publications for T-Web studies for the resulting classification to match the visual impression of the cosmic web \citep{Forero-Romero2009}.

The accuracy of the best model is $69\%$. Although the environments represented with a larger number of galaxies present a higher completeness. 
The environments ranked from higher to lower completeness are filaments ($80\%$), peaks ($63\%$), sheets ($59\%$), and voids ($6\%$).
We showed that higher completeness and F1 scores correlate with the number of instances in each class. 
A larger number of instances make it easier for the algorithm to find the relevant features for its correct classification, boosting the F1 score.

We compared our results against a similar methodology proposed by \cite{Tsizh2019}. 
They used dark matter haloes and fixed linking length network metrics to predict the web environment.
Their best model achieved a $70\%$ accuracy predicting the cosmic web environment computed with the CLASSIC algorithm using an extreme gradient boosting decision trees method. 
However, we showed how a direct comparison against our method is unfair given the radically different results from the CLASSIC and T-Web environment classification algorithms.

\cite{Tsizh2019} also tried to predict the T-web environment achieving only a $51\%$ accuracy.
This means that our method achieves $18$ percent points more 
in the prediction of the same kind of web environment.
This result could be explained by the through exploration we made over the parameter space to find the best values for
the smoothing length, eigenvalue threshold, and absolute magnitude cut.
But also  by the higher number densities of tracers we have access to, or the use of the $\beta$-skeleton compared to a fixed linking length network.

Our results provide the baseline for future work that will have to quantify the effect of redshift space distortions and survey incompleteness to predict the dark matter T-Web form survey data.
In order to facilitate future comparison and reuse of our results, we make our best-trained model public, together with the test data in this paper.
The model and the dataset can be found at \url{https://github.com/jsuarez314/cosmicweb_bsk}.

\section*{Data Availability}
The datasets were derived from sources in the public domain: Illustris-TNG project, \url{https://www.tng-project.org/}.
The test dataset and the best model presented in this paper are available at \url{https://github.com/jsuarez314/cosmicweb_bsk}.

\section*{Acknowledgements}

This work has been partially supported by the LACEGAL network with support from the European Union's Horizon 2020 Research and Innovation programme under the Marie Sklodowska-Curie grant agreement number 734374.

We are thankful to the community developing and maintaining open source packages fundamental to our work: numpy
\&  scipy  \citep{VanDerWalt2011},  the  Jupyter  notebook  \citep{Kluyver2016}, matplotlib \citep{Hunter2007}, corner.py \citep{Foreman-Mackey2016} and  scikit-learn \citep{Pedregosa2011}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%%%%%%
% The best way to enter references is to use BibTeX:
\bibliographystyle{mnras}
\bibliography{references}

% Alternatively you could enter them by hand, like this:
% This method is tedious and prone to error if you have lots of references
%\begin{thebibliography}{99}
%\end{thebibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%% APPENDICES %%%%%%%%%%%%%%%%%%%%%
%\appendix
%\section{Some extra material}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Don't change these lines
\bsp	% typesetting comment
\label{lastpage}
\end{document}
% End of mnras_template.tex